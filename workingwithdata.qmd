# Importing and cleaning data {#sec-data}

## Creating R data objects

We're able to work with data in R as objects.  You define an object by selecting a name, and assigning it some sort of value.  We'll illustrate this with a few different kinds of data, then later we'll instead work with reading in data from files.

We assign an object a value using the `<-` sequence.  Let's try.  After assigning your new object a value, you can navigate to it in the R object viewer in your IDE (R Studio or Visual Studio Code).  To display in this guide, we'll use `print()`.

```{R}
myobject <- 2

print(myobject)
```

We can store other kinds of data, like text, which is called a string.  Strings are always `"wrapped in quotes"` to be handled properly.

```{R}
mysecondobject <- "this is a string"

print(mysecondobject)
```

And of course, we can store more complex datasets than single values.  We can create a list with the special combine `c()` function.  You can think of a list as a row of data:

```{R}
mythirdobject <- c(1, 2, 3)

print(mythirdobject)
```

Our R object explorer might be starting to get a little cluttered with these example objects.  You can remove them in the GUI or you can run the `rm()` command to delete them:

```{R}
rm(myobject)
rm(mysecondobject)
rm(mythirdobject)
```

## Importing data

Now that we've learned how to work with R data objects, let's discuss working with actual data.  First, of course, we'll need to get the data into R.  How we get the data into R depends on how the data is stored and where the data is stored.  Let's consider each:

### By file format

Most data files are stored in one of two common formats: Comma-separated values (`.csv`) and Microsoft Excel files (`.xlsx`).  There are great packages in R to handle these kinds of imports.

First, `readr`, includes functions to import data from those `.csv` files, primarily `read_csv()`.  You'll need to load the `readr` library, then pass it a filename or URL.

::: {.callout-warning}
Remember, you'll need to run `install.packages()` to install the package before you can load it.  See @sec-introduction for details on installing packages.
:::

```{R}
#| eval: false
library("readr")

mydata <- read_csv("mydatafile.csv")
```

For modern Excel files (`.xlsx`), we use `read_xlsx()` from the `readxl` package.

```{R}
#| output: false
library("readxl")
```

```{R}
#| eval: false
mydata <- read_xlsx("mydatafile.xlsx")
```


### By file location

The other important difference is where your data files reside.  If you have the data locally, you can move or copy the files into your project directory.  When they are in your project directory, you can simply pass the filename to the appropriate function (e.g. `read_csv()` or `read_xlsx()`).

If the file exists on the internet, you may be able to pass the URL (in `""` quotes) to a function like `read_csv()`.  You can also use the `curl` package to download the file using `curl_download()` and set a filename for the location on your computer, then follow the instructions for a local file above.

The `curl` package can also be used to work with FTP/SFTP servers.  Since you'll need to pass credentials to the server in code, you'll want to look into the R environment file as detailed in @sec-renvironment.

```{R}
#| output: false
library("curl")
```

### From a database

If your data is stored in a database, it's likely that you'll be able to connect to your database using R.  The [`DBI` package](https://dbi.r-dbi.org/) details how to connect to your database, either by using ODBC or by specifying a driver.  The `DBI` package includes such functions as `dbReadTable()` to read in a data table and `dbGetQuery()` to run a query and grab the result.

### From an Application Program Interface (API)

If the data can be provided from a webservice, it's likely that you can use R to engage with the data.  This is beyond the scope of this guide, but packages exist for many popular public data sources that have APIs - for example, you can use [the `tidycensus` package](https://walker-data.com/tidycensus/) to access the [Census Bureau data APIs](https://www.census.gov/data/developers/data-sets.html).

## Tidy data

For much of our data cleaning, manipulation, and analysis in R we're going to use a collection of packages known as `tidyverse` [@tidyverse].  The `readr` package described above is actually part of the compilation!  This set of packages is designed to for data science, with a common design and underlying data structures.  We can load the entire collection with just one command:

```{R}
#| output: false
library("tidyverse")
```

`tidyverse` is named after a concept of what it means to have **tidy data**, which @wickham_tidy_2014 defines as:

1. Each variable is a column; each column is a variable.
2. Each observation is a row; each row is an observation.
3. Each value is a cell; each cell is a single value.

As institutional researchers, we do come across data that do not meet those requirements from time to time, especially when collected by humans, such as data coded as `semester 1`, `semester 2`, etc.  `tidyverse` includes functions, like `pivot_longer()` and `pivot_wider()`.  @wickham2023r has a great overview of these concepts and functions.

For this guide, we'll work with data that comes from adminstrative data sources, and already meets the tidy data standards, as it was collected and processed in machine readable formats.  That doesn't mean it won't need some cleaning, of course.


## In Practice: Dataset #1 -- IPEDS data {#sec-dataset1}

For working with our first dataset, we'll of course begin with data from the Integrated Postsecondary Education Data System (IPEDS).  Though many IR professionals are used to working with the web interface of the IPEDS data center, we also have the opportunity to interact with the complete data files, which are available as comma-separated values files (`.csv`) in a compressed format (`.zip`), or as Access databases.  We'll be working with the `.csv` files for now.^[Yes, there are a couple packages that are designed for accessing IPEDS data including [this one in CRAN](https://cran.r-project.org/web/packages/IPEDS/index.html) and [this one on GitHub](https://github.com/jbryer/ipeds).  But we're using the files directly from the IPEDS data center because we are focused on learning how to import and clean data.]

If we go to the IPEDS Data Center and click on "Complete Data Files", we'll reach this page: <https://nces.ed.gov/ipeds/datacenter/DataFiles.aspx>.  We can hover over the various file links and note that they share a similar stem and file format.  Let's set two variables to start: one to store this URL stem and one to store the latest year available:

```{R}
IPEDS_url <- "https://nces.ed.gov/ipeds/datacenter/data/"
IPEDS_year <- 2022
```

Next we're going to load libraries we'll need to use.  `tidyverse` should be loaded already but we'll load it again.  The `curl` package will assist us in downloading files. 

```{R}
#| output: false
# load libraries
library("tidyverse")
library("curl")
```

Let's use the simple `curl_download()` function from the `curl` package.  We need to provide it two parameters: a URL to download and a filename for the downloaded file.  We can build both of those by concatenating info to match the available links using the variables we stored above.  We'll grab the IPEDS Directory Information data, which comes from the Institutional Characteristics Header component ("HD"), and the Completions component data ("C").  We can create the filenames and URLs by concatenating text fields.  In Excel we would use `CONCATENATE()` or the special concatenation character (`&`), but using tidyverse we will use `str_c()`:

```{R}
#| freeze: true
curl_download(
  str_c(IPEDS_url, "HD", IPEDS_year, ".zip"),
  destfile = str_c("HD", IPEDS_year, ".zip")
)

curl_download(
  str_c(IPEDS_url, "C", IPEDS_year, "_A", ".zip"),
  destfile = str_c("C", IPEDS_year, "_A", ".zip")
)
```

Next we need to import the downloaded files into R and begin cleaning the data.  We'll start with the directory information data.  We'll begin using `read_csv()`, which we can use to read in the `.csv` file contained in the `.zip` file.  We'll store the data as `institutions`, then take a peek at the imported data using `glimse()`:

```{R}
institutions <- read_csv(
    str_c("HD", IPEDS_year, ".zip")
)

glimpse(institutions)
```

That's a lot of data!  Since we're not using all of the variables, let's use select to pull out a subset of them.  We'll use the `select()` function to do this.

::: {.callout-important}
This is an important place to teach an R technique that makes code much easier and cleaner to write.  If you look at the documentation for different functions, you'll note that typically, the first parameter of a function is a data object to use.

If we have several things we need to do with a data object, we could end up writing the name of the data object over, and over, and over, cluttering up our code.

R uses a special character sequence `|>`, called the pipe, that takes the output of one command and uses it as the following command.  (Some older documentation uses the sequence `%>%`, which also works, but `|>` is now preferred.)
:::

```{R}
institutions <- institutions |>
    select(UNITID, INSTNM, OPEID, STABBR, SECTOR, HLOFFER)

glimpse(institutions)
```

Let's move on to importing and cleaning the IPEDS Completions data.  We'll again use `read_csv()` to read in the data and store it in an object called `completions`.  Since we're familiar with the completions data, we know that we really only need major #1 and can filter out secondary majors.  We will ask `filter()` to keep only those rows equal to 1 (`== 1`).  Finally, similar to above, we only care about a few of the variables for now.  

```{R}
completions <- read_csv(
    str_c("C", IPEDS_year, "_A.zip")
    ) |>
    filter(MAJORNUM == 1) |>
    select(UNITID, CIPCODE, AWLEVEL, CTOTALT)

glimpse(completions)
```

Now that we have clean directory info (`institutions`) and clean completions data (`completions`), let's combine them.  We'll use the `*_join()` functions from the `tidyverse` `dplyr` package.  This is like joining tables in SQL, or for those of you who use Excel like databases, like using `VLOOKUP()`.

We'll store this combined dataset as `combined` for now.  We need to provide the two data objects we're joining, and the variable(s) we want to join on:

```{R}
combined <- left_join(
    institutions,
    completions,
    by = "UNITID",
    keep = TRUE
)

glimpse(combined)
```

::: {.callout-tip}
Note that when we get to more complex functions that would create really long lines of code or nested functions, it's best to separate your code into multiple lines to keep it readable.  R will ignore the line breaks that you create just like it ignores spaces.
:::

The data set is combined and clean, but since it has a row for every credential (or at least unique combinations of level and CIP code) for every institution, it's pretty large.  We'll work with Ohio data so we can filter again to Ohio institutions:

```{R}
combined <- combined |>
    filter(STABBR == "OH")

glimpse(combined)
```

Much better.  But we have two more steps we'd like to do.  Note that many of the variables are repeated in the dataset.  We can use a special data type, factors, to clean up this data into categorical variables.  `tidyverse` includes a package, `forcats`, to help with working with categorical data using the factor data type.  Let's do this for `SECTOR`, `AWLEVEL`, and `CTOTALT`:

```{R}
combined <- combined |>
    mutate(
        SECTOR = as_factor(SECTOR),
        HLOFFER = as_factor(HLOFFER),
        AWLEVEL = as_factor(AWLEVEL)
    )

glimpse(combined)
```

Lastly, since we want to use the `SECTOR` variable in particular later, let's provide it with desciptions that are used in IPEDS:^[And included in the dictionary file or the web interface]

```{R}
combined <- combined |>
    mutate(
        SECTOR = fct_recode(
            SECTOR,
            "Administrative Unit" = "0",
            "Public 4-year or above" = "1",
            "Private nonprofit 4-year or above" = "2",
            "Private for-profit 4-year or above" = "3",
            "Public 2-year" = "4",
            "Private nonprofit 2-year" = "5",
            "Private for-profit 2-year" = "6",
            "Public less-than-2-year" = "7",
            "Private nonprofit less-than-2-year" = "8",
            "Private for-profit less-than-2-year" = "9"
        )
    )
```

::: {.callout-note}
Once you're comfortable with many of these cleaning steps, you can revisit the `readr` function options.  You can actually change column types, select variables, etc. all with optional arguments.
:::

### Dataset #1 complete code

```{R}
#| eval: false
# load libraries
library("tidyverse")
library("curl")

# store IPEDS url & data year for use
IPEDS_url <- "https://nces.ed.gov/ipeds/datacenter/data/"
IPEDS_year <- 2022

# download data
curl_download(
  str_c(IPEDS_url, "HD", IPEDS_year, ".zip"),
  destfile = str_c("HD", IPEDS_year, ".zip")
)

curl_download(
  str_c(IPEDS_url, "C", IPEDS_year, "_A", ".zip"),
  destfile = str_c("C", IPEDS_year, "_A", ".zip")
)

# process institution directory information
institutions <- read_csv(
    str_c("HD", IPEDS_year, ".zip")
    ) |>
    select(UNITID, INSTNM, OPEID, STABBR, SECTOR, HLOFFER)

# process completions data
completions <- read_csv(
    str_c("C", IPEDS_year, "_A.zip")
    ) |>
    filter(MAJORNUM == 1) |>
    select(UNITID, CIPCODE, AWLEVEL, CTOTALT)

# combine datasets, additional cleaning
combined <- left_join(
    institutions,
    completions,
    by = "UNITID",
    keep = TRUE
    ) |>
    filter(STABBR == "OH") |> #filter to Ohio
    mutate(
        SECTOR = as_factor(SECTOR),
        HLOFFER = as_factor(HLOFFER),
        AWLEVEL = as_factor(AWLEVEL)
    ) |>
    mutate(
        SECTOR = fct_recode(
            SECTOR,
            "Administrative Unit" = "0",
            "Public 4-year or above" = "1",
            "Private nonprofit 4-year or above" = "2",
            "Private for-profit 4-year or above" = "3",
            "Public 2-year" = "4",
            "Private nonprofit 2-year" = "5",
            "Private for-profit 2-year" = "6",
            "Public less-than-2-year" = "7",
            "Private nonprofit less-than-2-year" = "8",
            "Private for-profit less-than-2-year" = "9"
        )
    )

```

## In Practice: Dataset #2 -- Occupation Projections data {#sec-dataset2}

For our second example dataset, we'll work with Occupational Projections data from the Ohio Department of Jobs and Family Services, Bureau of Labor Market Information.  The Long-Term Projections data by JobsOhio region and Metropolitan Statistical Area (MSA) can be found at <https://ohiolmi.com/Home/Projections/ProjectionsHome_08.16.23>.  As interest in post-graduate outcomes increases throughout higher education, institutional researchers are increasingly working with labor market data.

We'll begin with the Central Ohio Excel file.  If you copy the URL, you'll get ,<https://ohiolmi.com/_docs/PROJ/JobsOhio/Central.xlsx>.

::: {.callout-note}
If you hover over the file links, you'll notice that they share a common URL stem (`https://ohiolmi.com/_docs/PROJ/JobsOhio/`), which will be helpful to use in parameterized reports, covered in @sec-reports.
:::

Let's store that URL to make it easier to use:

```{R}
projections_url <- "https://ohiolmi.com/_docs/PROJ/JobsOhio/Central.xlsx"
```

We'll pull down the file with curl:

```{R}
curl_download(projections_url, "projectionsdata.xlsx")
```

Next we'll try to import the file as a data object, and then check the results with `summary()` and `glimpse()`:

```{R}
projections_data <- read_xlsx("projectionsdata.xlsx")
glimpse(projections_data)
summary(projections_data)
```

That returned a bunch of junk, because there are multiple header rows in the file.  Luckily, there's an optional parameter we can add to `read_xlsx` to skip to the line we want (line 6 has the headers we want), which we can read about in the [`readxl` documentation](https://readxl.tidyverse.org/reference/read_excel.html).

```{R}
projections_data <- read_xlsx("projectionsdata.xlsx", skip = 5)
glimpse(projections_data)
summary(projections_data)
```

It seems we have a little more data cleaning to do.  We need to change the types of a few columns^[By default, the tidyverse `read_` functions guess at column types by examining the fields, but it is imperfect.], rename a few columns, and do some filtering to remove the summary occupations.  Let's use additional parameters in `read_xlsx` to define types and column names.  By providing the column names instead of importing them, we need to change the `skip = ` option to 6 instead of 5.

::: {.callout-note}
There's often more than one way to do something.  Instead of providing column names on the import (and changing the skip parameter), we could rename the columns with the `rename()` function after import.
:::

```{R}
#| warning: false
projections_data <- read_xlsx(
    "projectionsdata.xlsx",
    skip = 6,
    col_names = c(
        "SOC",
        "Occupation",
        "Current_jobs",
        "Projected_jobs",
        "Change_num",
        "Change_pct",
        "Growth",
        "Exits",
        "Transfers",
        "Tot_openings",
        "Wage",
        "Notes"
    ),
    col_types = c(
        "text",
        "text",
        "numeric",
        "numeric",
        "numeric",
        "numeric",
        "numeric",
        "numeric",
        "numeric",
        "numeric",
        "numeric",
        "text"
    )
)

glimpse(projections_data)
summary(projections_data)
```

Next, let's filter the dataset to remove those summary occupations and any notes at the end.

Summary occupations are coded with SOC codes ending in "0000", so we can quickly identify them.  In Excel, we'd likely use the `MID()` or `RIGHT()` commands to pull out that sequence.  In R's tidyverse package, we can use `str_sub()`, which works very similar to these, extracting a subset of the string field based on character position.  Negative values mean work from the end.

Any rows with anything other than the SOC code in the SOC column should be ignored.  Since SOC codes are 7 characters long, we'll try to use that.  In Excel, we'd use `LEN()` to get the length, while here we'll use `str_length()` from the tidyverse package.  Note that in R, equals and not equals (`==` and `!=`) are different than in Excel (`=` and `<>`)

```{R}
projections_data <- projections_data |>
    filter(
        str_sub(SOC, -4, -1) != "0000",
        str_length(SOC) == 7
        )

glimpse(projections_data)
summary(projections_data)
```

And finally, we need to adjust the wage column.  It seems that there is a mix of hourly and annual wage figures in this column.  Let's convert all of them to annual wages as a new variable, by multiplying any values below $200/hr by 2,080 hours/yr.  We'll do this by using `mutate()` to create the new variable, and define it using a `case_when()`.^[`case_when()` is inspired by the SQL `CASE` statement, and is more elegant than nested `IF()` functions you may be used to using in Excel.] 


```{R}
projections_data <- projections_data |>
    mutate(
        annual_wage = case_when(
            Wage < 200 ~ Wage * 2080,
            .default = Wage
        )
    )

glimpse(projections_data)
summary(projections_data)
```

And now we have a clean dataset!  We'll use this further in @sec-analysis and @sec-reports, so let's show what it looks like all together.

### Dataset #2 complete code

```{R}
#| eval: false
# load libraries
library("readxl")
library("curl")
library("tidyverse")

# define file url
projections_url <- "https://ohiolmi.com/_docs/PROJ/JobsOhio/Central.xlsx"

# download file
curl_download(projections_url, "projectionsdata.xlsx")

# read file with approrpiate settings
projections_data <- read_xlsx(
    "projectionsdata.xlsx",
    skip = 6,
    col_names = c(
        "SOC",
        "Occupation",
        "Current_jobs",
        "Projected_jobs",
        "Change_num",
        "Change_pct",
        "Growth",
        "Exits",
        "Transfers",
        "Tot_openings",
        "Wage",
        "Notes"
    ),
    col_types = c(
        "text",
        "text",
        "numeric",
        "numeric",
        "numeric",
        "numeric",
        "numeric",
        "numeric",
        "numeric",
        "numeric",
        "numeric",
        "text"
    )
    ) |>

    # remove summary occupations and notes/blank rows
    filter(
        str_sub(SOC, -4, -1) != "0000",
        str_length(SOC) == 7
        ) |>
        
    # create annual wage column so values are consistent
    mutate(
        annual_wage = case_when(
            Wage < 200 ~ Wage * 2080,
            .default = Wage
        )
    )
    
```

## Exercises {.unnumbered}

### Exercise 1

Think about the kinds of data that you work with regularly.  Is it tidy?  How is it stored?  What functions will you need to use to work with it in R?

### Exercise 2

We used functions from several tidyverse packages.  Especially when learning, it's nice to have quick references.  Tidyverse has a series of official cheat sheets that you'll likely find useful.  Take a look:

| package | what this package is about | site | cheat sheet |
|---------|----------------------------|------|-------------|
| readxl | data import (esp. Excel files) | [site](https://readxl.tidyverse.org/) | [cheat sheet](https://github.com/rstudio/cheatsheets/blob/main/data-import.pdf) |
| dplyr | data transformation | [site](https://dplyr.tidyverse.org) | [cheat sheet](https://github.com/rstudio/cheatsheets/blob/main/data-transformation.pdf) |
| stringr | working with text | [site](https://stringr.tidyverse.org/) | [cheat sheet](https://github.com/rstudio/cheatsheets/blob/main/strings.pdf) |
| factors | categorical data | [site](https://forcats.tidyverse.org) | [cheat sheet](https://github.com/rstudio/cheatsheets/blob/main/factors.pdf) |


## Extra: Exporting data {.unnumbered}

At the start of this chapter we covered importing data - but didn't cover exporting data.  Of course, while this guide will show you how far you can take R, it also can be just one small piece of your toolchain if you wish.  You could use R to do some cleaning, processing, and some analysis, and then export the data for use in another tool like a dashboard product.

The `tidyverse` package `readr` includes a set of `write_` functions to export to CSV quickly. `write_csv_excel()` takes a data object input and a filename (e.g. `mydata.csv`) and builds out the file which can be read in Excel or other tools.

We briefly reviewed accessing a database earlier in this chapter.  `DBI` provides `dbWriteTable()` to write a data table object as a database table.  You must provide the db connection object, a table name, and then the data object you're storing.  It can take an `append` argument if you want to simply add data to an existing table.